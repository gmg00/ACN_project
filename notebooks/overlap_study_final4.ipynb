{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import networkx as nx\n",
    "from scipy.stats import pearsonr\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_index2(users_items, item_rarity, item_collections, alpha=0.5, collection_rarity=None):\n",
    "    \"\"\"\n",
    "    Calculates edge weights (similarity indices) between user pairs with normalization\n",
    "    and dynamic threshold adjustment.\n",
    "    \n",
    "    Args:\n",
    "        users_items (dict): Maps users to their collectible items.\n",
    "        item_rarity (dict): Maps items to their rarity (lower = rarer).\n",
    "        item_collections (dict): Maps items to their collections.\n",
    "        alpha (float): Weight for collection-based similarity (default: 0.5).\n",
    "        collection_rarity (dict): Maps collections to their rarity (optional).\n",
    "    \n",
    "    Returns:\n",
    "        list: List of tuples (user1, user2, normalized weight).\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "\n",
    "    # Find all user pairs\n",
    "    user_pairs = combinations(users_items.keys(), 2)\n",
    "\n",
    "    for user1, user2 in user_pairs:\n",
    "        # Get the items of each user\n",
    "        items1 = set(users_items[user1])\n",
    "        items2 = set(users_items[user2])\n",
    "\n",
    "        if not items1 or not items2:\n",
    "            continue\n",
    "\n",
    "        # Compute shared items and collections\n",
    "        common_items = items1.intersection(items2)\n",
    "        collections1 = {item_collections[item] for item in items1}\n",
    "        collections2 = {item_collections[item] for item in items2}\n",
    "        common_collections = collections1.intersection(collections2)\n",
    "\n",
    "        if not common_items and not common_collections:\n",
    "            continue\n",
    "\n",
    "        # Compute item rarity-based weight\n",
    "        weight_rarity = sum(1 / item_rarity[item] for item in common_items)\n",
    "\n",
    "        # Compute collection similarity weight\n",
    "        if collection_rarity:\n",
    "            weight_collections = sum(1 / collection_rarity[collection] for collection in common_collections)\n",
    "        else:\n",
    "            weight_collections = len(common_collections)\n",
    "\n",
    "        # Calculate raw similarity index\n",
    "        raw_weight = weight_rarity + alpha * weight_collections\n",
    "\n",
    "        # Normalize weight\n",
    "        max_possible_rarity = sum(1 / item_rarity[item] for item in items1.union(items2))\n",
    "        max_possible_collections = len(collections1.union(collections2)) if not collection_rarity else \\\n",
    "                                   sum(1 / collection_rarity[collection] for collection in collections1.union(collections2))\n",
    "        max_possible_weight = max_possible_rarity + alpha * max_possible_collections\n",
    "        normalized_weight = raw_weight / max_possible_weight\n",
    "\n",
    "        # Dynamic threshold adjustment\n",
    "        threshold = 0.  # Example threshold scaling\n",
    "        if normalized_weight >= threshold:\n",
    "            edges.append((user1, user2, normalized_weight))\n",
    "\n",
    "    return edges\n",
    "\n",
    "\n",
    "def compute_edges2(all_wearables, alpha=0.5, wearable_to_collection=None):\n",
    "    \"\"\"\n",
    "    Computes edges with normalized weights and dynamic threshold adjustment between users.\n",
    "    \n",
    "    Args:\n",
    "        all_wearables (dict): Maps users to their wearables.\n",
    "        alpha (float): Weight for collection-based similarity (default: 0.5).\n",
    "    \n",
    "    Returns:\n",
    "        list: List of edges (user1, user2, normalized weight).\n",
    "    \"\"\"\n",
    "    users_items = {}\n",
    "    item_rarity = {}\n",
    "    item_collections = {}\n",
    "    collection_rarity = {}\n",
    "\n",
    "    # Populate users_items, item_rarity, and item_collections\n",
    "    for user, wearables in all_wearables.items():\n",
    "        users_items[user] = []\n",
    "        for wearable in wearables:\n",
    "            nft_name = wearable['nft_name']\n",
    "            if wearable_to_collection:\n",
    "                nft_collection = wearable_to_collection[nft_name]\n",
    "            else:\n",
    "                nft_collection = wearable['nft_collection']\n",
    "\n",
    "            item_rarity[nft_name] = item_rarity.get(nft_name, 0) + 1\n",
    "            collection_rarity[nft_collection] = collection_rarity.get(nft_collection, 0) + 1\n",
    "            users_items[user].append(nft_name)\n",
    "            item_collections[nft_name] = nft_collection\n",
    "\n",
    "    # Compute edges with the new similarity function\n",
    "    edges = calculate_similarity_index2(users_items, item_rarity, item_collections, alpha, collection_rarity)\n",
    "\n",
    "    return edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/HP/Desktop/UNI/LM_1/ACN/ACN_project/data/address_to_wearables.json'\n",
    "with open(file_path, \"r\") as json_file:\n",
    "    address_to_wearables = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "new_address_to_wearables = {}\n",
    "num_wearables = []\n",
    "\n",
    "for address, wearables in address_to_wearables.items():\n",
    "    num_wearables.append(len(wearables))\n",
    "    if len(wearables) < k:\n",
    "        continue\n",
    "    new_address_to_wearables[address] = wearables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "\n",
    "edges = compute_edges2(new_address_to_wearables, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an array of weights\n",
    "weights = []\n",
    "\n",
    "for edge in edges:\n",
    "    weights.append(edge[2])\n",
    "\n",
    "weights = np.array(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_layer2(edges, threshold):\n",
    "    G = nx.Graph()\n",
    "    for edge in edges:\n",
    "        if edge[2] > threshold:\n",
    "            G.add_edge(edge[0], edge[1], weight = edge[2]) \n",
    "    return G\n",
    "\n",
    "threshold = np.percentile(weights, 90)\n",
    "layer2 = create_layer2(edges, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import ast\n",
    "\n",
    "def create_layer1_threshold(edges_dict, k_value, undirected=False):\n",
    "    \"\"\"\n",
    "    Create a network layer by filtering edges based on a threshold and normalizing weights using min-max normalization.\n",
    "\n",
    "    Args:\n",
    "        edges_dict (dict): Dictionary containing edge data, where keys are edge tuples (as strings)\n",
    "                           and values are dictionaries with weight data.\n",
    "        threshold (float): Threshold for including edges in the graph.\n",
    "        undirected (bool): Whether the graph is undirected or directed.\n",
    "\n",
    "    Returns:\n",
    "        nx.Graph or nx.DiGraph: A graph object containing edges that pass the threshold.\n",
    "    \"\"\"\n",
    "    # Step 1: Extract all weights to find min and max values\n",
    "    if undirected:\n",
    "        weights = [edge_data['simmetric_weight'] for edge_data in edges_dict.values()]\n",
    "    else:\n",
    "        weights = [\n",
    "            weight \n",
    "            for edge_data in edges_dict.values() \n",
    "            for weight in (edge_data['weight_user1'], edge_data['weight_user2'])\n",
    "        ]\n",
    "    threshold = np.percentile(np.array(weights), k_value)\n",
    "    # Compute min and max weights for normalization\n",
    "    w_min = min(weights)\n",
    "    w_max = max(weights)\n",
    "\n",
    "    # Avoid division by zero in case all weights are the same\n",
    "    if w_max == w_min:\n",
    "        normalize = lambda w: 1.0  # Set all normalized weights to 1.0\n",
    "    else:\n",
    "        normalize = lambda w: (w - w_min) / (w_max - w_min)  # Min-max normalization\n",
    "\n",
    "    threshold = (threshold -w_min) / (w_max - w_min)\n",
    "    # Step 2: Create the graph\n",
    "    if undirected:\n",
    "        G = nx.Graph()\n",
    "        for edge, edge_data in edges_dict.items():\n",
    "            e = ast.literal_eval(edge)\n",
    "            normalized_weight = normalize(edge_data['simmetric_weight'])\n",
    "            if normalized_weight > threshold:\n",
    "                G.add_edge(e[0], e[1], weight=normalized_weight)\n",
    "    else:\n",
    "        G = nx.DiGraph()\n",
    "        for edge, edge_data in edges_dict.items():\n",
    "            e = ast.literal_eval(edge)\n",
    "            normalized_weight_user1 = normalize(edge_data['weight_user1'])\n",
    "            normalized_weight_user2 = normalize(edge_data['weight_user2'])\n",
    "            if normalized_weight_user1 > threshold:\n",
    "                G.add_edge(e[0], e[1], weight=normalized_weight_user1)\n",
    "            if normalized_weight_user2 > threshold:\n",
    "                G.add_edge(e[1], e[0], weight=normalized_weight_user2)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def get_percentile(edges, k_value):\n",
    "    weights = []\n",
    "    for edge, edge_dict in edges.items():\n",
    "        weights.append(edge_dict['simmetric_weight'])\n",
    "    return np.percentile(np.array(weights), k_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/HP/Desktop/UNI/LM_1/ACN/ACN_project/data/edges_dict_weight.json'\n",
    "with open(file_path, \"r\") as json_file:\n",
    "    edges_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = create_layer1_threshold(edges_dict, 90, undirected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_keyword(string, keywords):\n",
    "    return any(keyword in string for keyword in keywords)\n",
    "\n",
    "def create_edges(all_events, keywords):\n",
    "    edges = []\n",
    "    addresses = list(all_events.keys())\n",
    "    for address, events in all_events.items():\n",
    "        for event in events:\n",
    "            nft_description = event.get('nft_description')\n",
    "\n",
    "            if nft_description and \"DCL Wearable\" in nft_description:\n",
    "                continue\n",
    "\n",
    "            if event.get('nft_name') and contains_keyword(event.get('nft_name'), keywords):\n",
    "                continue\n",
    "\n",
    "            if event.get('from') == address and event.get('to') in addresses and event.get('to') != address:\n",
    "\n",
    "                edges.append([address, event.get('to'), 1])\n",
    "            if event.get('seller') == address and event.get('buyer') in addresses and event.get('buyer') != address:\n",
    "\n",
    "                edges.append([address, event.get('buyer'), 1+event.get('price', 0)])\n",
    "\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/HP/Desktop/UNI/LM_1/ACN/ACN_project/data/address_to_events_final.json'\n",
    "with open(file_path, \"r\") as json_file:\n",
    "    address_to_events = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating wearables windows for each address\n",
    "keywords = [\"decentraland\", \"dcl\", \"decentral\", \"wearable\", \"decentral-games\", \"parcel\", \"MANA\", 'Decentraland']\n",
    "edges = create_edges(address_to_events, keywords=keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_dict = {}\n",
    "for edge in edges:\n",
    "    e = tuple(set(edge[0:2]))\n",
    "    if e not in edges_dict.keys():\n",
    "        edges_dict[e] = 1\n",
    "    else:\n",
    "        edges_dict[e] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer3 = nx.Graph()\n",
    "weights = []\n",
    "for edge, weight in edges_dict.items():\n",
    "    weights.append(weight)\n",
    "    layer3.add_edge(edge[0], edge[1], weight=weight)\n",
    "min_weight = np.min(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "def null_model_2(\n",
    "    G, nswap, target_sum, min_weight, max_tries=100, rel_tol=1e-6\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform double edge swaps on a graph while preserving edge weights and adjusting them \n",
    "    so that the sum of edge weights for each node matches the target sum.\n",
    "\n",
    "    Parameters:\n",
    "        G (nx.Graph): Input graph with weights on edges.\n",
    "        nswap (int): Number of swaps to perform.\n",
    "        target_sum (dict): Dictionary mapping each node to its target sum of edge weights.\n",
    "        min_weight (float): Minimum allowed weight for any edge.\n",
    "        max_tries (int): Maximum number of attempts to perform the swaps.\n",
    "        rel_tol (float): Relative tolerance for error in weight adjustment.\n",
    "\n",
    "    Returns:\n",
    "        nx.Graph: A graph with rewired edges and adjusted weights.\n",
    "    \"\"\"\n",
    "    # Step 1: Perform Edge Swaps\n",
    "    #t0 = time.time()\n",
    "    nx.double_edge_swap(G, nswap=nswap, max_tries=max_tries)\n",
    "    # print(f'Rewiring time: {time.time() - t0}')\n",
    "    # t0 = time.time()\n",
    "    # Step 2: Initialize weights (if not present)\n",
    "    for u, v in G.edges():\n",
    "        if 'weight' not in G[u][v]:\n",
    "            G[u][v]['weight'] = max(min_weight, 1.0)  # Default to at least min_weight\n",
    "\n",
    "    # Step 3: Iteratively adjust weights to match target sum\n",
    "    for iteration in range(100):  # Limit the number of iterations for adjustments\n",
    "        current_sum = {node: 0 for node in G.nodes()}\n",
    "        for u, v, data in G.edges(data=True):\n",
    "            current_sum[u] += data['weight']\n",
    "            current_sum[v] += data['weight']\n",
    "\n",
    "        # Compute relative errors and adjust weights\n",
    "        max_relative_error = 0\n",
    "        for u, v, data in G.edges(data=True):\n",
    "            # Calculate adjustment factors for u and v\n",
    "            scale_u = target_sum[u] / current_sum[u] if current_sum[u] > 0 else 1.0\n",
    "            scale_v = target_sum[v] / current_sum[v] if current_sum[v] > 0 else 1.0\n",
    "\n",
    "            # Adjust weight proportionally to both scales\n",
    "            new_weight = data['weight'] * (scale_u + scale_v) / 2.0\n",
    "            new_weight = max(min_weight, new_weight)  # Enforce minimum weight\n",
    "            max_relative_error = max(\n",
    "                abs(current_sum[u] - target_sum[u]) / target_sum[u],\n",
    "                abs(current_sum[v] - target_sum[v]) / target_sum[v]\n",
    "            )\n",
    "\n",
    "            # Update the edge weight\n",
    "            data['weight'] = new_weight\n",
    "\n",
    "        # Break if the maximum relative error is below tolerance\n",
    "        if max_relative_error < rel_tol:\n",
    "            break\n",
    "    # print(f'max_relative_error = {max_relative_error}')\n",
    "    # print(f'Adjustment time: {time.time() - t0}')\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_degree_distribution(original_dd, null_model_dd):\n",
    "    for degree, frequency in original_dd.items():\n",
    "        if degree not in null_model_dd and frequency != 0:\n",
    "            return False\n",
    "        if frequency != null_model_dd[degree]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr \n",
    "\n",
    "def calculate_layer_correlation(layer1, layer2):\n",
    "    edges1 = {(u, v): d['weight'] for u, v, d in layer1.edges(data=True)}\n",
    "    edges2 = {(u, v): d['weight'] for u, v, d in layer2.edges(data=True)}\n",
    "\n",
    "    common_edges = set(edges1.keys()).intersection(set(edges2.keys()))\n",
    "    weights1 = [edges1[edge] for edge in common_edges]\n",
    "    weights2 = [edges2[edge] for edge in common_edges]\n",
    "    #print(np.cov(weights1, weights2))\n",
    "\n",
    "    return spearmanr(weights1, weights2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def analyze_layer_correlation(layer1, layer2, weights1, weights2):\n",
    "    \"\"\"\n",
    "    Analyze correlation and overlap between two layers.\n",
    "\n",
    "    Args:\n",
    "        layer1 (set): Edge set of the first layer (e.g., {(u, v), ...}).\n",
    "        layer2 (set): Edge set of the second layer.\n",
    "        weights1 (dict): Weights of edges in the first layer, e.g., {(u, v): weight}.\n",
    "        weights2 (dict): Weights of edges in the second layer, e.g., {(u, v): weight}.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metrics including Spearman correlation, edge overlap, and composite score.\n",
    "    \"\"\"\n",
    "    # Find common edges\n",
    "    common_edges = layer1 & layer2\n",
    "    num_common_edges = len(common_edges)\n",
    "    \n",
    "    # Edge overlap ratio\n",
    "    edge_overlap_ratio = num_common_edges / min(len(layer1), len(layer2))\n",
    "    \n",
    "    # Extract weights for common edges\n",
    "    common_weights1 = [weights1[edge] for edge in common_edges]\n",
    "    common_weights2 = [weights2[edge] for edge in common_edges]\n",
    "    \n",
    "    # Compute Spearman correlation\n",
    "    if len(common_edges) > 1:\n",
    "        correlation, _ = spearmanr(common_weights1, common_weights2)\n",
    "    else:\n",
    "        correlation = 0  # Not enough data for correlation\n",
    "    \n",
    "    # Composite score (adjust alpha as needed)\n",
    "    alpha = 0.5\n",
    "    composite_score = alpha * correlation + (1 - alpha) * edge_overlap_ratio\n",
    "    \n",
    "    return {\n",
    "        \"correlation\": correlation,\n",
    "        \"edge_overlap_ratio\": edge_overlap_ratio,\n",
    "        \"composite_score\": composite_score,\n",
    "        \"num_common_edges\": num_common_edges,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges (layer 2): 55052\n",
      "Number of nodes (layer2): 2599\n",
      "Number of edges (layer 3): 1936\n",
      "Number of nodes (layer3): 799\n",
      "Density (layer2): 0.006072753849580146\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of edges (layer 2): {layer1.number_of_edges()}')\n",
    "print(f'Number of nodes (layer2): {layer1.number_of_nodes()}')\n",
    "print(f'Number of edges (layer 3): {layer3.number_of_edges()}')\n",
    "print(f'Number of nodes (layer3): {layer3.number_of_nodes()}')\n",
    "print(f'Density (layer2): {nx.density(layer3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarebbe opportuno fare almeno 10N swaps (N Ã¨ il numero di edges del network). Nel nostro caso N = 18592 quindi eseguiamo 200000 swaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n",
      "Iteration 25\n",
      "Iteration 26\n",
      "Iteration 27\n",
      "Iteration 28\n",
      "Iteration 29\n",
      "Iteration 30\n",
      "Iteration 31\n",
      "Iteration 32\n",
      "Iteration 33\n",
      "Iteration 34\n",
      "Iteration 35\n",
      "Iteration 36\n",
      "Iteration 37\n",
      "Iteration 38\n",
      "Iteration 39\n",
      "Iteration 40\n",
      "Iteration 41\n",
      "Iteration 42\n",
      "Iteration 43\n",
      "Iteration 44\n",
      "Iteration 45\n",
      "Iteration 46\n",
      "Iteration 47\n",
      "Iteration 48\n",
      "Iteration 49\n",
      "Iteration 50\n",
      "Iteration 51\n",
      "Iteration 52\n",
      "Iteration 53\n",
      "Iteration 54\n",
      "Iteration 55\n",
      "Iteration 56\n",
      "Iteration 57\n",
      "Iteration 58\n",
      "Iteration 59\n",
      "Iteration 60\n",
      "Iteration 61\n",
      "Iteration 62\n",
      "Iteration 63\n",
      "Iteration 64\n",
      "Iteration 65\n",
      "Iteration 66\n",
      "Iteration 67\n",
      "Iteration 68\n",
      "Iteration 69\n",
      "Iteration 70\n",
      "Iteration 71\n",
      "Iteration 72\n",
      "Iteration 73\n",
      "Iteration 74\n",
      "Iteration 75\n",
      "Iteration 76\n",
      "Iteration 77\n",
      "Iteration 78\n",
      "Iteration 79\n",
      "Iteration 80\n",
      "Iteration 81\n",
      "Iteration 82\n",
      "Iteration 83\n",
      "Iteration 84\n",
      "Iteration 85\n",
      "Iteration 86\n",
      "Iteration 87\n",
      "Iteration 88\n",
      "Iteration 89\n",
      "Iteration 90\n",
      "Iteration 91\n",
      "Iteration 92\n",
      "Iteration 93\n",
      "Iteration 94\n",
      "Iteration 95\n",
      "Iteration 96\n",
      "Iteration 97\n",
      "Iteration 98\n",
      "Iteration 99\n"
     ]
    }
   ],
   "source": [
    "scores23 = []\n",
    "scores13 = []\n",
    "N = 100\n",
    "target_sum = {u: sum(layer3[u][v]['weight'] for v in layer3.neighbors(u)) for u in layer3.nodes()}\n",
    "for i in range(N):\n",
    "    print(f'Iteration {i}')\n",
    "    layer3_copy = layer3.copy()\n",
    "    null_model = null_model_2(layer3_copy, nswap=20000, target_sum = target_sum, max_tries=500000, min_weight = 1, rel_tol = 0.05)\n",
    "   \n",
    "    weights1 = {(u, v): d['weight'] for u, v, d in layer1.edges(data=True)}\n",
    "    weights2 = {(u, v): d['weight'] for u, v, d in layer2.edges(data=True)}\n",
    "\n",
    "    weights_null = {(u, v): d['weight'] for u, v, d in null_model.edges(data=True)}\n",
    "    \n",
    "    score13 = analyze_layer_correlation(set(layer1.edges()), set(null_model.edges()), weights1, weights_null)\n",
    "    score23 = analyze_layer_correlation(set(layer2.edges()), set(null_model.edges()), weights2, weights_null)\n",
    "    scores13.append(score13)\n",
    "    scores23.append(score23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.33333333333333"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5000/60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.35 secondi a iterazione -> circa 6 minuti per 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/HP/Desktop/UNI/LM_1/ACN/ACN_project/data/scores13.json'\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(scores13, json_file)\n",
    "\n",
    "file_path = '/Users/HP/Desktop/UNI/LM_1/ACN/ACN_project/data/scores23.json'\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(scores23, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlap study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/HP/Desktop/UNI/LM_1/ACN/ACN_project/data/scores_carlo.json'\n",
    "with open(file_path, \"r\") as json_file:\n",
    "    scores = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = []\n",
    "edge_overlap_ratios = []\n",
    "composite_scores = []\n",
    "for elem in scores13:\n",
    "    composite_scores.append(elem['composite_score'])\n",
    "    correlations.append(elem['correlation'])\n",
    "    edge_overlap_ratios.append(elem['edge_overlap_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x1c4c4f6bb50>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.hist(correlations, bins=30, alpha=0.7, label=\"Null Model\", color='lightblue', edgecolor='black')\n",
    "expected_correlation = calculate_layer_correlation(layer1, layer3)[0]\n",
    "plt.axvline(expected_correlation, color='red', linestyle='--', label=\"Observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected edge overlap ratio: 0.11570247933884298\n",
      "Mean edge overlap ratio: 0.04987086776859504\n",
      "Standard deviation edge overlap ratio: 0.004312552555832865\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "plt.hist(edge_overlap_ratios, bins=30, alpha=0.7, label=\"Null Model\", color='lightblue', edgecolor='black')\n",
    "expected_commons = len(set(layer3.edges()).intersection(set(layer1.edges())))\n",
    "expected_edge_overlap_ratio = expected_commons / min(layer3.number_of_edges(), layer1.number_of_edges())\n",
    "print(f'Expected edge overlap ratio: {expected_edge_overlap_ratio}')\n",
    "print(f'Mean edge overlap ratio: {np.mean(edge_overlap_ratios)}')\n",
    "print(f'Standard deviation edge overlap ratio: {np.std(edge_overlap_ratios)}')\n",
    "plt.xlabel('Edge-overlap ratio', size=25)\n",
    "plt.ylabel('Frequency', size=25)\n",
    "plt.title('Edge-overlap ratio Distribution', size=30)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid()\n",
    "plt.axvline(np.mean(edge_overlap_ratios), color='green', linestyle='--', label=f\"Mean = {np.mean(edge_overlap_ratios):.2f}\")\n",
    "plt.axvline(expected_edge_overlap_ratio, color='red', linestyle='--', label=f\"Observed = {expected_edge_overlap_ratio:.2f}\")\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## z-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_z_value(null_model_ratios, original_ratio):\n",
    "    \"\"\"\n",
    "    Computes the z-value for the original edge overlap ratio compared to null models.\n",
    "    \n",
    "    Parameters:\n",
    "    - null_model_ratios (array-like): Array of edge overlap ratios from null models.\n",
    "    - original_ratio (float): Edge overlap ratio from the original dataset.\n",
    "    \n",
    "    Returns:\n",
    "    - z_value (float): The z-value of the original ratio.\n",
    "    \"\"\"\n",
    "    # Calculate the mean and standard deviation of the null models\n",
    "    mean_null = np.mean(null_model_ratios)\n",
    "    std_null = np.std(null_model_ratios)  # Use ddof=1 for sample standard deviation\n",
    "    \n",
    "    # Compute the z-value\n",
    "    z_value = (original_ratio - mean_null) / (std_null/np.sqrt(len(null_model_ratios)))\n",
    "    \n",
    "    return z_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z-value: 1392.0958523076044\n"
     ]
    }
   ],
   "source": [
    "expected_commons = len(set(layer3.edges()).intersection(set(layer2.edges())))\n",
    "expected_edge_overlap_ratio = expected_commons / min(layer3.number_of_edges(), layer2.number_of_edges())\n",
    "print(f'z-value: {compute_z_value(edge_overlap_ratios, expected_edge_overlap_ratio)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_weights = []\n",
    "for u, v in layer3.edges():\n",
    "        before_weights.append(layer3[u][v]['weight'])\n",
    "\n",
    "after_weights = []\n",
    "for u, v in null_model.edges():\n",
    "        after_weights.append(null_model[u][v]['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 12))\n",
    "plt.subplot(2,1,1)\n",
    "plt.hist(before_weights, bins=100, color='lightblue', alpha=0.7, edgecolor='black', label='Weights')\n",
    "plt.title('Weight Distribution - Original vs Null Model', fontsize=20)\n",
    "plt.xlabel('Original Weights', fontsize=15)\n",
    "plt.ylabel('Frequency', fontsize=15)\n",
    "plt.yscale('log')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(alpha=0.4)\n",
    "plt.subplot(2,1,2)\n",
    "plt.hist(after_weights, bins=100, color='lightblue', alpha=0.7, edgecolor='black', label='Weights')\n",
    "\n",
    "plt.xlabel('Null Model Weights', fontsize=15)\n",
    "plt.ylabel('Frequency', fontsize=15)\n",
    "plt.yscale('log')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(alpha=0.4)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
